---
---

@article{huettenrauch2024robust,
  author  = {Maximilian H{{\"u}}ttenrauch and Gerhard Neumann},
  title   = {Robust Black-Box Optimization for Stochastic Search and Episodic Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {153},
  pages   = {1--44},
  html    = {http://jmlr.org/papers/v25/22-0564.html},
  selected={true},
  preview = {more_jmlr.png}
}

@article{parras2021deep,
AUTHOR = {Parras, Juan and Hüttenrauch, Maximilian and Zazo, Santiago and Neumann, Gerhard},
TITLE = {Deep Reinforcement Learning for Attacking Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4060},
html = {https://www.mdpi.com/1424-8220/21/12/4060},
PubMedID = {34204726},
ISSN = {1424-8220},
ABSTRACT = {Recent advances in Deep Reinforcement Learning allow solving increasingly complex problems. In this work, we show how current defense mechanisms in Wireless Sensor Networks are vulnerable to attacks that use these advances. We use a Deep Reinforcement Learning attacker architecture that allows having one or more attacking agents that can learn to attack using only partial observations. Then, we subject our architecture to a test-bench consisting of two defense mechanisms against a distributed spectrum sensing attack and a backoff attack. Our simulations show that our attacker learns to exploit these systems without having a priori information about the defense mechanism used nor its concrete parameters. Since our attacker requires minimal hyper-parameter tuning, scales with the number of attackers, and learns only by interacting with the defense mechanism, it poses a significant threat to current defense procedures.},
DOI = {10.3390/s21124060}
}

@article{huettenrauch2019deep,
  author  = {Maximilian H{{\"u}}ttenrauch and Adrian {\v{S}}o{\v{s}}i{{\'c}} and Gerhard Neumann},
  title   = {Deep Reinforcement Learning for Swarm Systems },
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {54},
  pages   = {1--31},
  html    = {http://jmlr.org/papers/v20/18-476.html},
  selected={true},
  preview = {mfe_jmlr.png}
}

@inproceedings{huettenrauch2018local,
author="H{\"u}ttenrauch, Maximilian
and {\v{S}}o{\v{s}}i{\'{c}}, Adrian
and Neumann, Gerhard",
editor="Dorigo, Marco
and Birattari, Mauro
and Blum, Christian
and Christensen, Anders L.
and Reina, Andreagiovanni
and Trianni, Vito",
title="Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning",
booktitle="Swarm Intelligence",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="71--83",
abstract="Swarm systems constitute a challenging problem for reinforcement learning (RL) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents. While it is often difficult to directly define the behavior of the agents, simple communication protocols can be defined more easily using prior knowledge about the given task. In this paper, we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment. The protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information, such as the shortest distance and direction to a desired target. In our framework, we use an adaptation of Trust Region Policy Optimization to learn complex collaborative tasks, such as formation building and building a communication link. We evaluate our findings in a simulated 2D-physics environment, and compare the implications of different communication protocols.",
isbn="978-3-030-00533-7"
}


@inproceedings{
dahlinger2023informationtheoretic,
title={Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization},
author={Philipp Dahlinger and Philipp Becker and Maximilian H{\"u}ttenrauch and Gerhard Neumann},
booktitle={OPT 2023: Optimization for Machine Learning},
year={2023},
html={https://openreview.net/forum?id=Y02MmhKEhu}
}

@inproceedings{huettenrauch2021coordinate,
author = {H\"{u}ttenrauch, Maximilian and Neumann, Gerhard},
title = {Coordinate ascent MORE with adaptive entropy control for population-based regret minimization},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
html = {https://doi.org/10.1145/3449726.3463183},
doi = {10.1145/3449726.3463183},
abstract = {Model-based Relative Entropy Policy Search (MORE) is a population-based stochastic search algorithm with desirable properties such as a well defined policy search objective, i.e., it optimizes the expected return, and exact closed form information theoretic update rules. This is in contrast with existing population-based methods, that are often referred to as evolutionary strategies, such as CMA-ES. While these methods work very well in practice, the updates of the search distribution are often based on heuristics and they do not optimize the expected return of the population but instead implicitly optimize the return of elite samples, which may yield a poor expected return and unreliable or risky solutions. We show that the MORE algorithm can be improved with distinct updates based on coordinate ascent on the mean and covariance of the search distribution, which considerably improves the convergence speed while maintaining the exact closed form updates. In this way, we can match the performance of elite samples of CMA-ES while also showing a considerably improved performance of the sample average. We evaluate our new algorithm on simulated robotic tasks and compare to the state of the art CMA-ES.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1493–1497},
numpages = {5},
keywords = {policy search, reinforcement learning, robotics, stochastic search},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{huettenrauch2017guided,
	author = {Hüttenrauch, Maximilian and {\v S}o{\v s}i{\'c}, Adrian and Neumann, Gerhard},
	title = {Guided Deep Reinforcement Learning for Swarm Systems},
	booktitle = {Autonomous Robots and Multirobot Systems (ARMS) Workshop, AAMAS 2017},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Systems and Control, Statistics - Machine Learning},
	year = {2017},
    location={São Paulo, Brazil},
	keywords={workshop},
	arxiv={1709.06011}
}